{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing requirements"
      ],
      "metadata": {
        "id": "s9TWiSIKHyJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy nltk matplotlib seaborn plotly scikit-learn\n",
        "!pip install kaleido\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zljqNDxkd-nC",
        "outputId": "5b6526f8-d005-4011-a93c-a7bf4ec349b2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: choreographer>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from kaleido) (1.0.9)\n",
            "Requirement already satisfied: logistro>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from kaleido) (1.1.0)\n",
            "Requirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.11/dist-packages (from kaleido) (3.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kaleido) (25.0)\n",
            "Requirement already satisfied: simplejson>=3.19.3 in /usr/local/lib/python3.11/dist-packages (from choreographer>=1.0.5->kaleido) (3.20.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Comparison"
      ],
      "metadata": {
        "id": "eeJbVMAQH2x_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsFrBglid8DD",
        "outputId": "8b21e531-1c94-4e62-f921-955dcde338d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Model Comparison...\n",
            "============================================================\n",
            "Loading dataset...\n",
            "Original dataset size: 50,000\n",
            "After cleaning: 49,582\n",
            "Using 20,000 reviews for analysis\n",
            "Sentiment distribution:\n",
            "  positive: 10,012 (50.1%)\n",
            "  negative: 9,988 (49.9%)\n",
            "Processing reviews...\n",
            "Processed 1,000 reviews\n",
            "Processed 2,000 reviews\n",
            "Processed 3,000 reviews\n",
            "Processed 4,000 reviews\n",
            "Processed 5,000 reviews\n",
            "Processed 6,000 reviews\n",
            "Processed 7,000 reviews\n",
            "Processed 8,000 reviews\n",
            "Processed 9,000 reviews\n",
            "Processed 10,000 reviews\n",
            "Processed 11,000 reviews\n",
            "Processed 12,000 reviews\n",
            "Processed 13,000 reviews\n",
            "Processed 14,000 reviews\n",
            "Processed 15,000 reviews\n",
            "Processed 16,000 reviews\n",
            "Processed 17,000 reviews\n",
            "Processed 18,000 reviews\n",
            "Processed 19,000 reviews\n",
            "Processed 20,000 reviews\n",
            "Final dataset size: 20,000\n",
            "Vectorizing text...\n",
            "Applying feature scaling...\n",
            "Data preparation completed successfully\n",
            "\n",
            "============================================================\n",
            "TRAINING AND EVALUATING MODELS\n",
            "============================================================\n",
            "\n",
            "Training Logistic Regression...\n",
            "‚úì Logistic Regression completed in 82.68s - Accuracy: 0.8393\n",
            "\n",
            "Training Linear SVM...\n",
            "‚úì Linear SVM completed in 39.60s - Accuracy: 0.8045\n",
            "\n",
            "Training Naive Bayes...\n",
            "‚úì Naive Bayes completed in 1.27s - Accuracy: 0.8555\n",
            "\n",
            "Training Random Forest...\n",
            "‚úì Random Forest completed in 30.06s - Accuracy: 0.8297\n",
            "\n",
            "Training Neural Network...\n",
            "‚úì Neural Network completed in 113.51s - Accuracy: 0.8585\n",
            "\n",
            "================================================================================\n",
            "DETAILED MODEL COMPARISON REPORT\n",
            "================================================================================\n",
            "\n",
            "Overall Performance Ranking:\n",
            "--------------------------------------------------------------------------------\n",
            "              Model  Accuracy  Precision  Recall  F1-Score  Training Time (s)\n",
            "     Neural Network    0.8585     0.8596  0.8585    0.8584           113.5132\n",
            "        Naive Bayes    0.8555     0.8562  0.8555    0.8554             1.2724\n",
            "Logistic Regression    0.8393     0.8393  0.8393    0.8392            82.6770\n",
            "      Random Forest    0.8297     0.8327  0.8297    0.8294            30.0637\n",
            "         Linear SVM    0.8045     0.8045  0.8045    0.8045            39.6020\n",
            "\n",
            "üèÜ BEST MODEL: Neural Network\n",
            "   Accuracy: 0.8585\n",
            "   Precision: 0.8596\n",
            "   Recall: 0.8585\n",
            "   F1-Score: 0.8584\n",
            "   Training Time: 113.51s\n",
            "\n",
            "üéâ FINAL RECOMMENDATION: Use Neural Network with 85.9% accuracy\n",
            "\n",
            "============================================================\n",
            "RESULTS FOR STREAMLIT APP:\n",
            "============================================================\n",
            "MODEL_COMPARISON_RESULTS = {\n",
            "    'Logistic Regression': {'accuracy': 0.8393, 'training_time': 82.68, 'precision': 0.8393, 'recall': 0.8393, 'f1_score': 0.8392},\n",
            "    'Linear SVM': {'accuracy': 0.8045, 'training_time': 39.60, 'precision': 0.8045, 'recall': 0.8045, 'f1_score': 0.8045},\n",
            "    'Naive Bayes': {'accuracy': 0.8555, 'training_time': 1.27, 'precision': 0.8562, 'recall': 0.8555, 'f1_score': 0.8554},\n",
            "    'Random Forest': {'accuracy': 0.8297, 'training_time': 30.06, 'precision': 0.8327, 'recall': 0.8297, 'f1_score': 0.8294},\n",
            "    'Neural Network': {'accuracy': 0.8585, 'training_time': 113.51, 'precision': 0.8596, 'recall': 0.8585, 'f1_score': 0.8584},\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import re\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "class ImprovedModelComparison:\n",
        "    def __init__(self, sample_size=10000):\n",
        "        self.sample_size = sample_size\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            max_features=15000,\n",
        "            min_df=2,\n",
        "            max_df=0.95,\n",
        "            ngram_range=(1, 3),\n",
        "            stop_words='english',\n",
        "            sublinear_tf=True\n",
        "        )\n",
        "\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "        self.models = {\n",
        "            'Logistic Regression': LogisticRegression(\n",
        "                max_iter=2000,\n",
        "                random_state=42,\n",
        "                C=2.0,\n",
        "                solver='liblinear',\n",
        "                class_weight='balanced'\n",
        "            ),\n",
        "            'Linear SVM': LinearSVC(\n",
        "                max_iter=2000,\n",
        "                random_state=42,\n",
        "                C=1.0,\n",
        "                class_weight='balanced'\n",
        "            ),\n",
        "            'Naive Bayes': MultinomialNB(alpha=0.1),\n",
        "            'Random Forest': RandomForestClassifier(\n",
        "                n_estimators=100,\n",
        "                random_state=42,\n",
        "                max_depth=20,\n",
        "                min_samples_split=5,\n",
        "                class_weight='balanced'\n",
        "            ),\n",
        "            'Neural Network': MLPClassifier(\n",
        "                hidden_layer_sizes=(100, 50),\n",
        "                max_iter=1000,\n",
        "                random_state=42,\n",
        "                alpha=0.001\n",
        "            )\n",
        "        }\n",
        "\n",
        "        self.results = {}\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        if pd.isna(text) or text == '':\n",
        "            return ''\n",
        "\n",
        "        text = str(text).lower()\n",
        "\n",
        "        contractions = {\n",
        "            \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
        "            \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\",\n",
        "            \"'d\": \" would\", \"'m\": \" am\", \"it's\": \"it is\",\n",
        "            \"that's\": \"that is\", \"what's\": \"what is\", \"there's\": \"there is\"\n",
        "        }\n",
        "\n",
        "        for contraction, expansion in contractions.items():\n",
        "            text = text.replace(contraction, expansion)\n",
        "\n",
        "        text = re.sub(r'<[^>]+>', '', text)\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "        try:\n",
        "            tokens = word_tokenize(text)\n",
        "        except:\n",
        "            tokens = text.split()\n",
        "\n",
        "        negation_words = {\"not\", \"no\", \"never\", \"none\", \"nobody\", \"nothing\",\n",
        "                         \"neither\", \"nowhere\", \"hardly\", \"scarcely\", \"barely\"}\n",
        "\n",
        "        processed_tokens = []\n",
        "        negate = False\n",
        "\n",
        "        for i, token in enumerate(tokens):\n",
        "            if token in negation_words:\n",
        "                negate = True\n",
        "                processed_tokens.append(token)\n",
        "            elif negate and i < len(tokens):\n",
        "                processed_tokens.append(f\"NOT_{token}\")\n",
        "                negate = False\n",
        "            elif token not in self.stop_words or len(token) <= 2:\n",
        "                try:\n",
        "                    lemmatized = self.lemmatizer.lemmatize(token)\n",
        "                    if len(lemmatized) > 2:\n",
        "                        processed_tokens.append(lemmatized)\n",
        "                except:\n",
        "                    if len(token) > 2:\n",
        "                        processed_tokens.append(token)\n",
        "\n",
        "        return ' '.join(processed_tokens)\n",
        "\n",
        "    def prepare_data(self, data_path):\n",
        "        print(\"Loading dataset...\")\n",
        "        df = pd.read_csv(data_path)\n",
        "\n",
        "        print(f\"Original dataset size: {len(df):,}\")\n",
        "        df = df.drop_duplicates(subset=['review'])\n",
        "        df = df.dropna(subset=['review', 'sentiment'])\n",
        "        df = df[df['review'].str.len() > 0]\n",
        "        print(f\"After cleaning: {len(df):,}\")\n",
        "\n",
        "        if len(df) > self.sample_size:\n",
        "            df = df.sample(n=self.sample_size, random_state=42)\n",
        "\n",
        "        print(f\"Using {len(df):,} reviews for analysis\")\n",
        "\n",
        "        sentiment_counts = df['sentiment'].value_counts()\n",
        "        print(f\"Sentiment distribution:\")\n",
        "        for sentiment, count in sentiment_counts.items():\n",
        "            print(f\"  {sentiment}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "        processed_reviews = []\n",
        "        batch_size = 1000\n",
        "\n",
        "        print(\"Processing reviews...\")\n",
        "        for i in range(0, len(df), batch_size):\n",
        "            batch = df['review'].iloc[i:i+batch_size]\n",
        "            processed_batch = [self.preprocess_text(review) for review in batch]\n",
        "            processed_reviews.extend(processed_batch)\n",
        "            print(f\"Processed {min(i+batch_size, len(df)):,} reviews\")\n",
        "\n",
        "        df['processed_review'] = processed_reviews\n",
        "        df = df[df['processed_review'].str.len() > 0]\n",
        "        print(f\"Final dataset size: {len(df):,}\")\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            df['processed_review'],\n",
        "            df['sentiment'],\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=df['sentiment']\n",
        "        )\n",
        "\n",
        "        print(\"Vectorizing text...\")\n",
        "        X_train_vec = self.vectorizer.fit_transform(X_train)\n",
        "        X_test_vec = self.vectorizer.transform(X_test)\n",
        "\n",
        "        self.X_train_raw = X_train_vec.toarray()\n",
        "        self.X_test_raw = X_test_vec.toarray()\n",
        "\n",
        "        print(\"Applying feature scaling...\")\n",
        "        X_train_scaled = self.scaler.fit_transform(self.X_train_raw)\n",
        "        X_test_scaled = self.scaler.transform(self.X_test_raw)\n",
        "\n",
        "        print(\"Data preparation completed successfully\")\n",
        "        return X_train_scaled, X_test_scaled, y_train, y_test\n",
        "\n",
        "    def train_and_evaluate(self, X_train, X_test, y_train, y_test):\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"TRAINING AND EVALUATING MODELS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"\\nTraining {name}...\")\n",
        "            start_time = time()\n",
        "\n",
        "            try:\n",
        "\n",
        "                if name == 'Naive Bayes':\n",
        "                    # Naive Bayes works with raw TF-IDF (non-negative)\n",
        "                    X_train_model = self.X_train_raw\n",
        "                    X_test_model = self.X_test_raw\n",
        "                else:\n",
        "                    # Other models can use scaled data\n",
        "                    X_train_model = X_train\n",
        "                    X_test_model = X_test\n",
        "\n",
        "                model.fit(X_train_model, y_train)\n",
        "                y_pred = model.predict(X_test_model)\n",
        "                training_time = time() - start_time\n",
        "\n",
        "                class_report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
        "\n",
        "                self.results[name] = {\n",
        "                    'accuracy': accuracy_score(y_test, y_pred),\n",
        "                    'training_time': training_time,\n",
        "                    'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
        "                    'classification_report': class_report,\n",
        "                    'precision': class_report['weighted avg']['precision'],\n",
        "                    'recall': class_report['weighted avg']['recall'],\n",
        "                    'f1_score': class_report['weighted avg']['f1-score']\n",
        "                }\n",
        "\n",
        "                print(f\"‚úì {name} completed in {training_time:.2f}s - Accuracy: {self.results[name]['accuracy']:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚úó {name} failed: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    def generate_detailed_report(self):\n",
        "        if not self.results:\n",
        "            print(\"No results to report.\")\n",
        "            return None\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"DETAILED MODEL COMPARISON REPORT\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        comparison_data = []\n",
        "        for name, results in self.results.items():\n",
        "            comparison_data.append({\n",
        "                'Model': name,\n",
        "                'Accuracy': results['accuracy'],\n",
        "                'Precision': results['precision'],\n",
        "                'Recall': results['recall'],\n",
        "                'F1-Score': results['f1_score'],\n",
        "                'Training Time (s)': results['training_time']\n",
        "            })\n",
        "\n",
        "        df_comparison = pd.DataFrame(comparison_data)\n",
        "        df_comparison = df_comparison.sort_values('Accuracy', ascending=False)\n",
        "\n",
        "        print(\"\\nOverall Performance Ranking:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(df_comparison.to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "        if len(df_comparison) > 0:\n",
        "            best_model = df_comparison.iloc[0]\n",
        "            print(f\"\\nüèÜ BEST MODEL: {best_model['Model']}\")\n",
        "            print(f\"   Accuracy: {best_model['Accuracy']:.4f}\")\n",
        "            print(f\"   Precision: {best_model['Precision']:.4f}\")\n",
        "            print(f\"   Recall: {best_model['Recall']:.4f}\")\n",
        "            print(f\"   F1-Score: {best_model['F1-Score']:.4f}\")\n",
        "            print(f\"   Training Time: {best_model['Training Time (s)']:.2f}s\")\n",
        "\n",
        "        return df_comparison\n",
        "\n",
        "    def find_best_model(self):\n",
        "        if not self.results:\n",
        "            return None, None\n",
        "\n",
        "        best_accuracy = 0\n",
        "        best_model = None\n",
        "\n",
        "        for name, results in self.results.items():\n",
        "            if results['accuracy'] > best_accuracy:\n",
        "                best_accuracy = results['accuracy']\n",
        "                best_model = name\n",
        "\n",
        "        return best_model, best_accuracy\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Model Comparison...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        comparison = ImprovedModelComparison(sample_size=20000)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = comparison.prepare_data('IMDB_Dataset.csv')\n",
        "        comparison.train_and_evaluate(X_train, X_test, y_train, y_test)\n",
        "        df_comparison = comparison.generate_detailed_report()\n",
        "        best_model, best_accuracy = comparison.find_best_model()\n",
        "\n",
        "        if best_model:\n",
        "            print(f\"\\nüéâ FINAL RECOMMENDATION: Use {best_model} with {best_accuracy:.1%} accuracy\")\n",
        "\n",
        "        # Results for Streamlit\n",
        "        if comparison.results:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"RESULTS FOR STREAMLIT APP:\")\n",
        "            print(\"=\"*60)\n",
        "            print(\"MODEL_COMPARISON_RESULTS = {\")\n",
        "            for name, results in comparison.results.items():\n",
        "                print(f\"    '{name}': {{'accuracy': {results['accuracy']:.4f}, 'training_time': {results['training_time']:.2f}, 'precision': {results['precision']:.4f}, 'recall': {results['recall']:.4f}, 'f1_score': {results['f1_score']:.4f}}},\")\n",
        "            print(\"}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Error: {str(e)}\")"
      ]
    }
  ]
}